{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from help_printing import *\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['celeba_140000.npy', 'celeba_60000.npy', 'celeba_40000.npy', 'celeba_160000.npy', 'celeba_200000.npy', 'celeba_80000.npy', 'celeba_120000.npy', 'celeba_100000.npy', 'celeba_180000.npy', 'celeba_20000.npy']\n"
     ]
    }
   ],
   "source": [
    "data_path = 'data/celeba_split/'\n",
    "data_list = os.listdir(data_path)\n",
    "print(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 64, 64, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = [np.load(data_path+data) for data in data_list[:4]]\n",
    "#x_data = np.load(data_path+data_list[0])\n",
    "x_data = np.concatenate(dataset)\n",
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test = train_test_split(x_data, train_size = 0.9)\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, x_train)).shuffle(10000).batch(256)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, x_test)).batch(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 10), (None, 10),  230900    \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 64, 64, 3)         228803    \n",
      "=================================================================\n",
      "Total params: 459,703\n",
      "Trainable params: 459,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from Models import build_vae\n",
    "\n",
    "encoder, decoder, vae = build_vae()\n",
    "#encoder.summary()\n",
    "#decoder.summary()\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "train_loss =  tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "\n",
    "def get_rec_loss(inputs, predictions):\n",
    "    rec_loss = tf.keras.losses.mean_squared_error(inputs, predictions)\n",
    "    rec_loss *= 64*64\n",
    "    rec_loss = K.mean(rec_loss)\n",
    "    return rec_loss\n",
    "\n",
    "def get_kl_loss(z_log_var, z_mean):\n",
    "    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "    kl_loss = K.sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    return kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def plot_recimg(save_dir, epoch):\n",
    "    org_img = x_train[:100]\n",
    "    rec_img = vae(x_train[:100], training=False)\n",
    "    fig, ax = plt.subplots(6, 10, figsize=(20, 10))\n",
    "    for i in range(10):\n",
    "        for j in range(6):\n",
    "            if j%2 ==0: img=org_img \n",
    "            else: img=rec_img\n",
    "            ax[j][i].set_axis_off()\n",
    "            ax[j][i].imshow(img[10*(j//2)+i])\n",
    "    \n",
    "    plt.savefig('%s/recimg_%i.png'%(save_dir,epoch))\n",
    "    if epoch%show_term==0:\n",
    "        plt.show()\n",
    "    plt.close('all')\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        z_log_var, z_mean, z = encoder(inputs)\n",
    "        predictions = decoder(z)\n",
    "        \n",
    "        rec_loss = get_rec_loss(inputs, predictions)\n",
    "        kl_loss = get_kl_loss(z_log_var, z_mean)\n",
    "        loss = K.mean(rec_loss + kl_loss)\n",
    "    \n",
    "    varialbes = vae.trainable_variables\n",
    "    gradients = tape.gradient(loss, varialbes)\n",
    "    optimizer.apply_gradients(zip(gradients, varialbes))\n",
    "    \n",
    "    train_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'results_VAE/train_6'\n",
    "if_not_make(save_dir)\n",
    "epochs = 300\n",
    "checkpoint = tf.train.Checkpoint(step=tf.Variable(1), encoder=encoder, decoder=decoder, vae=vae)\n",
    "manager = tf.train.CheckpointManager(checkpoint, save_dir, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Epoch: 0, loss: 86.537964 \n",
      "* Epoch: 1, loss: 86.533669 \n",
      "* Epoch: 2, loss: 86.529282 \n",
      "* Epoch: 3, loss: 86.525078 \n",
      "* Epoch: 4, loss: 86.520752 \n",
      "* Epoch: 5, loss: 86.516350 \n",
      "* Epoch: 6, loss: 86.512047 \n",
      "* Epoch: 7, loss: 86.507675 \n",
      "* Epoch: 8, loss: 86.503471 \n",
      "* Epoch: 9, loss: 86.499283 \n",
      "* Epoch: 10, loss: 86.495171 \n",
      "* Epoch: 11, loss: 86.490944 \n",
      "* Epoch: 12, loss: 86.486610 \n",
      "* Epoch: 13, loss: 86.482376 \n",
      "* Epoch: 14, loss: 86.478195 \n",
      "* Epoch: 15, loss: 86.473869 \n",
      "* Epoch: 16, loss: 86.469635 \n",
      "* Epoch: 17, loss: 86.465492 \n",
      "* Epoch: 18, loss: 86.461372 \n",
      "* Epoch: 19, loss: 86.457199 \n",
      "* Epoch: 20, loss: 86.453171 \n",
      "* Epoch: 21, loss: 86.449219 \n",
      "* Epoch: 22, loss: 86.445107 \n",
      "* Epoch: 23, loss: 86.441124 \n",
      "* Epoch: 24, loss: 86.437057 \n",
      "* Epoch: 25, loss: 86.433174 \n",
      "* Epoch: 26, loss: 86.429108 \n",
      "* Epoch: 27, loss: 86.425255 \n",
      "* Epoch: 28, loss: 86.421295 \n",
      "* Epoch: 29, loss: 86.417336 \n",
      "* Epoch: 30, loss: 86.413567 \n",
      "* Epoch: 31, loss: 86.409584 \n",
      "* Epoch: 32, loss: 86.405602 \n",
      "* Epoch: 33, loss: 86.401665 \n",
      "* Epoch: 34, loss: 86.397675 \n",
      "* Epoch: 35, loss: 86.393753 \n",
      "* Epoch: 36, loss: 86.389870 \n",
      "* Epoch: 37, loss: 86.385979 \n",
      "* Epoch: 38, loss: 86.382133 \n",
      "* Epoch: 39, loss: 86.378334 \n",
      "* Epoch: 40, loss: 86.374481 \n",
      "* Epoch: 41, loss: 86.370789 \n",
      "* Epoch: 42, loss: 86.367012 \n",
      "* Epoch: 43, loss: 86.363220 \n",
      "* Epoch: 44, loss: 86.359413 \n",
      "* Epoch: 45, loss: 86.355637 \n",
      "* Epoch: 46, loss: 86.351822 \n",
      "* Epoch: 47, loss: 86.348167 \n",
      "* Epoch: 48, loss: 86.344315 \n",
      "* Epoch: 49, loss: 86.340576 \n",
      "* Epoch: 50, loss: 86.336807 \n",
      "* Epoch: 51, loss: 86.333153 \n",
      "* Epoch: 52, loss: 86.329399 \n",
      "* Epoch: 53, loss: 86.325821 \n",
      "* Epoch: 54, loss: 86.322250 \n",
      "* Epoch: 55, loss: 86.318550 \n",
      "* Epoch: 56, loss: 86.314796 \n",
      "* Epoch: 57, loss: 86.311218 \n",
      "* Epoch: 58, loss: 86.307472 \n",
      "* Epoch: 59, loss: 86.303810 \n",
      "* Epoch: 60, loss: 86.300262 \n",
      "* Epoch: 61, loss: 86.296616 \n",
      "* Epoch: 62, loss: 86.293030 \n",
      "* Epoch: 63, loss: 86.289482 \n",
      "* Epoch: 64, loss: 86.285919 \n",
      "* Epoch: 65, loss: 86.282402 \n",
      "* Epoch: 66, loss: 86.278793 \n",
      "* Epoch: 67, loss: 86.275215 \n",
      "* Epoch: 68, loss: 86.271622 \n",
      "* Epoch: 69, loss: 86.268158 \n",
      "* Epoch: 70, loss: 86.264717 \n",
      "* Epoch: 71, loss: 86.261230 \n",
      "* Epoch: 72, loss: 86.257759 \n",
      "* Epoch: 73, loss: 86.254204 \n",
      "* Epoch: 74, loss: 86.250687 \n",
      "* Epoch: 75, loss: 86.247238 \n",
      "* Epoch: 76, loss: 86.243797 \n",
      "* Epoch: 77, loss: 86.240387 \n",
      "* Epoch: 78, loss: 86.236961 \n",
      "* Epoch: 79, loss: 86.233582 \n",
      "* Epoch: 80, loss: 86.230125 \n",
      "* Epoch: 81, loss: 86.226700 \n",
      "* Epoch: 82, loss: 86.223267 \n",
      "* Epoch: 83, loss: 86.219933 \n",
      "* Epoch: 84, loss: 86.216637 \n",
      "* Epoch: 85, loss: 86.213272 \n",
      "* Epoch: 86, loss: 86.209969 \n",
      "* Epoch: 87, loss: 86.206635 \n",
      "* Epoch: 88, loss: 86.203346 \n",
      "* Epoch: 89, loss: 86.200127 \n",
      "* Epoch: 90, loss: 86.196777 \n",
      "* Epoch: 91, loss: 86.193520 \n",
      "* Epoch: 92, loss: 86.190208 \n",
      "* Epoch: 93, loss: 86.186821 \n",
      "* Epoch: 94, loss: 86.183525 \n",
      "* Epoch: 95, loss: 86.180237 \n",
      "* Epoch: 96, loss: 86.177078 \n",
      "* Epoch: 97, loss: 86.173843 \n",
      "* Epoch: 98, loss: 86.170540 \n",
      "* Epoch: 99, loss: 86.167168 \n",
      "* Epoch: 100, loss: 86.164139 \n",
      "* Epoch: 101, loss: 86.161057 \n",
      "* Epoch: 102, loss: 86.157860 \n",
      "* Epoch: 103, loss: 86.154648 \n",
      "* Epoch: 104, loss: 86.151489 \n",
      "* Epoch: 105, loss: 86.148300 \n",
      "* Epoch: 106, loss: 86.145187 \n",
      "* Epoch: 107, loss: 86.141945 \n",
      "* Epoch: 108, loss: 86.138809 \n",
      "* Epoch: 109, loss: 86.135735 \n",
      "* Epoch: 110, loss: 86.132713 \n",
      "* Epoch: 111, loss: 86.129547 \n",
      "* Epoch: 112, loss: 86.126389 \n",
      "* Epoch: 113, loss: 86.123322 \n",
      "* Epoch: 114, loss: 86.120262 \n",
      "* Epoch: 115, loss: 86.117256 \n",
      "* Epoch: 116, loss: 86.114105 \n",
      "* Epoch: 117, loss: 86.111046 \n",
      "* Epoch: 118, loss: 86.107956 \n",
      "* Epoch: 119, loss: 86.104866 \n",
      "* Epoch: 120, loss: 86.101799 \n",
      "* Epoch: 121, loss: 86.098785 \n",
      "* Epoch: 122, loss: 86.095680 \n",
      "* Epoch: 123, loss: 86.092728 \n",
      "* Epoch: 124, loss: 86.089745 \n",
      "* Epoch: 125, loss: 86.086716 \n",
      "* Epoch: 126, loss: 86.083771 \n",
      "* Epoch: 127, loss: 86.080696 \n",
      "* Epoch: 128, loss: 86.077766 \n",
      "* Epoch: 129, loss: 86.074692 \n",
      "* Epoch: 130, loss: 86.071701 \n",
      "* Epoch: 131, loss: 86.068703 \n",
      "* Epoch: 132, loss: 86.065636 \n",
      "* Epoch: 133, loss: 86.062744 \n",
      "* Epoch: 134, loss: 86.059769 \n",
      "* Epoch: 135, loss: 86.056847 \n",
      "* Epoch: 136, loss: 86.053940 \n",
      "* Epoch: 137, loss: 86.051064 \n",
      "* Epoch: 138, loss: 86.048149 \n",
      "* Epoch: 139, loss: 86.045212 \n",
      "* Epoch: 140, loss: 86.042358 \n",
      "* Epoch: 141, loss: 86.039429 \n",
      "* Epoch: 142, loss: 86.036621 \n",
      "* Epoch: 143, loss: 86.033775 \n",
      "* Epoch: 144, loss: 86.030884 \n",
      "* Epoch: 145, loss: 86.027985 \n",
      "* Epoch: 146, loss: 86.025223 \n",
      "* Epoch: 147, loss: 86.022308 \n",
      "* Epoch: 148, loss: 86.019432 \n",
      "* Epoch: 149, loss: 86.016693 \n",
      "* Epoch: 150, loss: 86.013809 \n",
      "* Epoch: 151, loss: 86.011009 \n",
      "* Epoch: 152, loss: 86.008224 \n",
      "* Epoch: 153, loss: 86.005424 \n",
      "* Epoch: 154, loss: 86.002632 \n",
      "* Epoch: 155, loss: 85.999786 \n",
      "* Epoch: 156, loss: 85.997078 \n",
      "* Epoch: 157, loss: 85.994324 \n",
      "* Epoch: 158, loss: 85.991646 \n",
      "* Epoch: 159, loss: 85.988953 \n",
      "* Epoch: 160, loss: 85.986145 \n",
      "* Epoch: 161, loss: 85.983383 \n",
      "* Epoch: 162, loss: 85.980667 \n",
      "* Epoch: 163, loss: 85.977959 \n",
      "* Epoch: 164, loss: 85.975327 \n",
      "* Epoch: 165, loss: 85.972580 \n",
      "* Epoch: 166, loss: 85.969833 \n",
      "* Epoch: 167, loss: 85.967079 \n",
      "* Epoch: 168, loss: 85.964424 \n",
      "* Epoch: 169, loss: 85.961693 \n",
      "* Epoch: 170, loss: 85.958961 \n",
      "* Epoch: 171, loss: 85.956299 \n",
      "* Epoch: 172, loss: 85.953735 \n",
      "* Epoch: 173, loss: 85.951042 \n",
      "* Epoch: 174, loss: 85.948364 \n",
      "* Epoch: 175, loss: 85.945694 \n",
      "* Epoch: 176, loss: 85.943054 \n",
      "* Epoch: 177, loss: 85.940483 \n",
      "* Epoch: 178, loss: 85.937820 \n",
      "* Epoch: 179, loss: 85.935188 \n",
      "* Epoch: 180, loss: 85.932625 \n",
      "* Epoch: 181, loss: 85.930008 \n",
      "* Epoch: 182, loss: 85.927505 \n",
      "* Epoch: 183, loss: 85.924896 \n",
      "* Epoch: 184, loss: 85.922302 \n",
      "* Epoch: 185, loss: 85.919662 \n",
      "* Epoch: 186, loss: 85.917007 \n",
      "* Epoch: 187, loss: 85.914474 \n",
      "* Epoch: 188, loss: 85.911850 \n",
      "* Epoch: 189, loss: 85.909355 \n",
      "* Epoch: 190, loss: 85.906754 \n",
      "* Epoch: 191, loss: 85.904144 \n",
      "* Epoch: 192, loss: 85.901543 \n",
      "* Epoch: 193, loss: 85.899040 \n",
      "* Epoch: 194, loss: 85.896469 \n",
      "* Epoch: 195, loss: 85.894005 \n",
      "* Epoch: 196, loss: 85.891632 \n",
      "* Epoch: 197, loss: 85.889076 \n",
      "* Epoch: 198, loss: 85.886581 \n",
      "* Epoch: 199, loss: 85.884064 \n",
      "* Epoch: 200, loss: 85.881485 \n",
      "* Epoch: 201, loss: 85.878975 \n",
      "* Epoch: 202, loss: 85.876450 \n",
      "* Epoch: 203, loss: 85.873970 \n",
      "* Epoch: 204, loss: 85.871445 \n",
      "* Epoch: 205, loss: 85.868988 \n",
      "* Epoch: 206, loss: 85.866539 \n",
      "* Epoch: 207, loss: 85.864067 \n",
      "* Epoch: 208, loss: 85.861656 \n",
      "* Epoch: 209, loss: 85.859245 \n",
      "* Epoch: 210, loss: 85.856850 \n",
      "* Epoch: 211, loss: 85.854340 \n",
      "* Epoch: 212, loss: 85.851875 \n",
      "* Epoch: 213, loss: 85.849464 \n",
      "* Epoch: 214, loss: 85.846985 \n",
      "* Epoch: 215, loss: 85.844597 \n",
      "* Epoch: 216, loss: 85.842155 \n",
      "* Epoch: 217, loss: 85.839752 \n",
      "* Epoch: 218, loss: 85.837349 \n",
      "* Epoch: 219, loss: 85.834938 \n",
      "* Epoch: 220, loss: 85.832497 \n",
      "* Epoch: 221, loss: 85.830124 \n",
      "* Epoch: 222, loss: 85.827744 \n",
      "* Epoch: 223, loss: 85.825340 \n",
      "* Epoch: 224, loss: 85.823006 \n",
      "* Epoch: 225, loss: 85.820656 \n",
      "* Epoch: 226, loss: 85.818222 \n",
      "* Epoch: 227, loss: 85.815811 \n",
      "* Epoch: 228, loss: 85.813568 \n",
      "* Epoch: 229, loss: 85.811226 \n",
      "* Epoch: 230, loss: 85.808891 \n",
      "* Epoch: 231, loss: 85.806526 \n",
      "* Epoch: 232, loss: 85.804176 \n",
      "* Epoch: 233, loss: 85.801842 \n",
      "* Epoch: 234, loss: 85.799583 \n",
      "* Epoch: 235, loss: 85.797295 \n",
      "* Epoch: 236, loss: 85.794991 \n",
      "* Epoch: 237, loss: 85.792709 \n",
      "* Epoch: 238, loss: 85.790436 \n",
      "* Epoch: 239, loss: 85.788162 \n",
      "* Epoch: 240, loss: 85.785866 \n",
      "* Epoch: 241, loss: 85.783638 \n",
      "* Epoch: 242, loss: 85.781563 \n",
      "* Epoch: 243, loss: 85.779297 \n",
      "* Epoch: 244, loss: 85.777084 \n",
      "* Epoch: 245, loss: 85.774780 \n",
      "* Epoch: 246, loss: 85.772537 \n",
      "* Epoch: 247, loss: 85.770256 \n",
      "* Epoch: 248, loss: 85.767921 \n",
      "* Epoch: 249, loss: 85.765625 \n",
      "* Epoch: 250, loss: 85.763390 \n",
      "* Epoch: 251, loss: 85.761162 \n",
      "* Epoch: 252, loss: 85.758934 \n",
      "* Epoch: 253, loss: 85.756607 \n",
      "* Epoch: 254, loss: 85.754272 \n",
      "* Epoch: 255, loss: 85.752083 \n",
      "* Epoch: 256, loss: 85.749870 \n",
      "* Epoch: 257, loss: 85.747536 \n",
      "* Epoch: 258, loss: 85.745277 \n",
      "* Epoch: 259, loss: 85.743050 \n",
      "* Epoch: 260, loss: 85.740845 \n",
      "* Epoch: 261, loss: 85.738747 \n",
      "* Epoch: 262, loss: 85.736519 \n",
      "* Epoch: 263, loss: 85.734329 \n",
      "* Epoch: 264, loss: 85.732094 \n",
      "* Epoch: 265, loss: 85.729935 \n",
      "* Epoch: 266, loss: 85.727745 \n",
      "* Epoch: 267, loss: 85.725624 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Epoch: 268, loss: 85.723404 \n",
      "* Epoch: 269, loss: 85.721275 \n",
      "* Epoch: 270, loss: 85.719048 \n",
      "* Epoch: 271, loss: 85.716911 \n",
      "* Epoch: 272, loss: 85.714767 \n",
      "* Epoch: 273, loss: 85.712646 \n",
      "* Epoch: 274, loss: 85.710510 \n",
      "* Epoch: 275, loss: 85.708237 \n",
      "* Epoch: 276, loss: 85.706093 \n",
      "* Epoch: 277, loss: 85.704033 \n",
      "* Epoch: 278, loss: 85.701843 \n",
      "* Epoch: 279, loss: 85.699791 \n",
      "* Epoch: 280, loss: 85.697647 \n",
      "* Epoch: 281, loss: 85.695473 \n",
      "* Epoch: 282, loss: 85.693382 \n",
      "* Epoch: 283, loss: 85.691315 \n",
      "* Epoch: 284, loss: 85.689339 \n",
      "* Epoch: 285, loss: 85.687248 \n",
      "* Epoch: 286, loss: 85.685188 \n",
      "* Epoch: 287, loss: 85.683220 \n",
      "* Epoch: 288, loss: 85.681076 \n",
      "* Epoch: 289, loss: 85.679001 \n",
      "* Epoch: 290, loss: 85.676918 \n",
      "* Epoch: 291, loss: 85.674858 \n",
      "* Epoch: 292, loss: 85.672798 \n",
      "* Epoch: 293, loss: 85.670685 \n",
      "* Epoch: 294, loss: 85.668571 \n",
      "* Epoch: 295, loss: 85.666443 \n",
      "* Epoch: 296, loss: 85.664452 \n",
      "* Epoch: 297, loss: 85.662407 \n",
      "* Epoch: 298, loss: 85.660378 \n",
      "* Epoch: 299, loss: 85.658325 \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for inputs, outputs in train_ds:\n",
    "        train_step(inputs)\n",
    "        \n",
    "    print(\"* Epoch: %i, loss: %f \"%(epoch, train_loss.result()))\n",
    "    manager.save()\n",
    "    plot_recimg(save_dir, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
