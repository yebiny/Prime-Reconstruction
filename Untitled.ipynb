{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from help_printing import *\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['celeba_140000.npy', 'celeba_60000.npy', 'celeba_40000.npy', 'celeba_160000.npy', 'celeba_200000.npy', 'celeba_80000.npy', 'celeba_120000.npy', 'celeba_100000.npy', 'celeba_180000.npy', 'celeba_20000.npy']\n"
     ]
    }
   ],
   "source": [
    "data_path = 'data/celeba_split/'\n",
    "data_list = os.listdir(data_path)\n",
    "print(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 64, 64, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = [np.load(data_path+data) for data in data_list[:4]]\n",
    "#x_data = np.load(data_path+data_list[0])\n",
    "x_data = np.concatenate(dataset)\n",
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test = train_test_split(x_data, train_size = 0.9)\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, x_train)).shuffle(10000).batch(256)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, x_test)).batch(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 10), (None, 10),  230900    \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 64, 64, 3)         228803    \n",
      "=================================================================\n",
      "Total params: 459,703\n",
      "Trainable params: 459,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from Models import build_vae\n",
    "\n",
    "encoder, decoder, vae = build_vae()\n",
    "#encoder.summary()\n",
    "#decoder.summary()\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "train_loss =  tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "\n",
    "def get_rec_loss(inputs, predictions):\n",
    "    rec_loss = tf.keras.losses.mean_squared_error(inputs, predictions)\n",
    "    rec_loss *= 64*64\n",
    "    rec_loss = K.mean(rec_loss)\n",
    "    return rec_loss\n",
    "\n",
    "def get_kl_loss(z_log_var, z_mean):\n",
    "    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "    kl_loss = K.sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    return kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def plot_recimg(save_dir, epoch):\n",
    "    org_img = x_train[:100]\n",
    "    rec_img = vae(x_train[:100], training=False)\n",
    "    fig, ax = plt.subplots(6, 10, figsize=(20, 10))\n",
    "    for i in range(10):\n",
    "        for j in range(6):\n",
    "            if j%2 ==0: img=org_img \n",
    "            else: img=rec_img\n",
    "            ax[j][i].set_axis_off()\n",
    "            ax[j][i].imshow(img[10*(j//2)+i])\n",
    "    \n",
    "    plt.savefig('%s/recimg_%i.png'%(save_dir,epoch))\n",
    "    #plt.show()\n",
    "    plt.close('all')\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        z_log_var, z_mean, z = encoder(inputs)\n",
    "        predictions = decoder(z)\n",
    "        \n",
    "        rec_loss = get_rec_loss(inputs, predictions)\n",
    "        kl_loss = get_kl_loss(z_log_var, z_mean)\n",
    "        loss = K.mean(rec_loss + kl_loss)\n",
    "    \n",
    "    varialbes = vae.trainable_variables\n",
    "    gradients = tape.gradient(loss, varialbes)\n",
    "    optimizer.apply_gradients(zip(gradients, varialbes))\n",
    "    \n",
    "    train_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'results_VAE/train_5'\n",
    "if_not_make(save_dir)\n",
    "epochs = 300\n",
    "checkpoint = tf.train.Checkpoint(step=tf.Variable(1), encoder=encoder, decoder=decoder, vae=vae)\n",
    "manager = tf.train.CheckpointManager(checkpoint, save_dir, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Epoch: 0, loss: 89.752388 \n",
      "* Epoch: 1, loss: 89.722214 \n",
      "* Epoch: 2, loss: 89.692825 \n",
      "* Epoch: 3, loss: 89.663979 \n",
      "* Epoch: 4, loss: 89.635452 \n",
      "* Epoch: 5, loss: 89.607536 \n",
      "* Epoch: 6, loss: 89.579430 \n",
      "* Epoch: 7, loss: 89.551949 \n",
      "* Epoch: 8, loss: 89.524406 \n",
      "* Epoch: 9, loss: 89.497520 \n",
      "* Epoch: 10, loss: 89.470558 \n",
      "* Epoch: 11, loss: 89.444824 \n",
      "* Epoch: 12, loss: 89.418747 \n",
      "* Epoch: 13, loss: 89.393196 \n",
      "* Epoch: 14, loss: 89.367386 \n",
      "* Epoch: 15, loss: 89.342560 \n",
      "* Epoch: 16, loss: 89.317436 \n",
      "* Epoch: 17, loss: 89.293327 \n",
      "* Epoch: 18, loss: 89.269669 \n",
      "* Epoch: 19, loss: 89.245941 \n",
      "* Epoch: 20, loss: 89.222412 \n",
      "* Epoch: 21, loss: 89.199074 \n",
      "* Epoch: 22, loss: 89.175934 \n",
      "* Epoch: 23, loss: 89.153168 \n",
      "* Epoch: 24, loss: 89.130676 \n",
      "* Epoch: 25, loss: 89.108467 \n",
      "* Epoch: 26, loss: 89.086380 \n",
      "* Epoch: 27, loss: 89.064499 \n",
      "* Epoch: 28, loss: 89.042534 \n",
      "* Epoch: 29, loss: 89.021202 \n",
      "* Epoch: 30, loss: 89.000252 \n",
      "* Epoch: 31, loss: 88.979195 \n",
      "* Epoch: 32, loss: 88.958473 \n",
      "* Epoch: 33, loss: 88.938400 \n",
      "* Epoch: 34, loss: 88.918236 \n",
      "* Epoch: 35, loss: 88.898232 \n",
      "* Epoch: 36, loss: 88.877876 \n",
      "* Epoch: 37, loss: 88.858101 \n",
      "* Epoch: 38, loss: 88.838966 \n",
      "* Epoch: 39, loss: 88.819809 \n",
      "* Epoch: 40, loss: 88.800568 \n",
      "* Epoch: 41, loss: 88.782043 \n",
      "* Epoch: 42, loss: 88.763702 \n",
      "* Epoch: 43, loss: 88.745102 \n",
      "* Epoch: 44, loss: 88.726654 \n",
      "* Epoch: 45, loss: 88.708626 \n",
      "* Epoch: 46, loss: 88.690842 \n",
      "* Epoch: 47, loss: 88.673447 \n",
      "* Epoch: 48, loss: 88.655640 \n",
      "* Epoch: 49, loss: 88.638466 \n",
      "* Epoch: 50, loss: 88.621178 \n",
      "* Epoch: 51, loss: 88.603813 \n",
      "* Epoch: 52, loss: 88.586571 \n",
      "* Epoch: 53, loss: 88.569801 \n",
      "* Epoch: 54, loss: 88.552895 \n",
      "* Epoch: 55, loss: 88.536591 \n",
      "* Epoch: 56, loss: 88.520187 \n",
      "* Epoch: 57, loss: 88.503799 \n",
      "* Epoch: 58, loss: 88.488205 \n",
      "* Epoch: 59, loss: 88.472366 \n",
      "* Epoch: 60, loss: 88.456764 \n",
      "* Epoch: 61, loss: 88.440933 \n",
      "* Epoch: 62, loss: 88.425621 \n",
      "* Epoch: 63, loss: 88.410576 \n",
      "* Epoch: 64, loss: 88.395432 \n",
      "* Epoch: 65, loss: 88.380516 \n",
      "* Epoch: 66, loss: 88.365860 \n",
      "* Epoch: 67, loss: 88.350876 \n",
      "* Epoch: 68, loss: 88.336090 \n",
      "* Epoch: 69, loss: 88.321831 \n",
      "* Epoch: 70, loss: 88.307304 \n",
      "* Epoch: 71, loss: 88.293350 \n",
      "* Epoch: 72, loss: 88.279343 \n",
      "* Epoch: 73, loss: 88.265327 \n",
      "* Epoch: 74, loss: 88.251144 \n",
      "* Epoch: 75, loss: 88.237099 \n",
      "* Epoch: 76, loss: 88.223366 \n",
      "* Epoch: 77, loss: 88.209930 \n",
      "* Epoch: 78, loss: 88.196152 \n",
      "* Epoch: 79, loss: 88.182800 \n",
      "* Epoch: 80, loss: 88.169403 \n",
      "* Epoch: 81, loss: 88.156158 \n",
      "* Epoch: 82, loss: 88.143044 \n",
      "* Epoch: 83, loss: 88.129913 \n",
      "* Epoch: 84, loss: 88.117119 \n",
      "* Epoch: 85, loss: 88.104362 \n",
      "* Epoch: 86, loss: 88.091461 \n",
      "* Epoch: 87, loss: 88.078621 \n",
      "* Epoch: 88, loss: 88.065933 \n",
      "* Epoch: 89, loss: 88.053764 \n",
      "* Epoch: 90, loss: 88.041451 \n",
      "* Epoch: 91, loss: 88.029190 \n",
      "* Epoch: 92, loss: 88.016747 \n",
      "* Epoch: 93, loss: 88.004669 \n",
      "* Epoch: 94, loss: 87.992500 \n",
      "* Epoch: 95, loss: 87.980469 \n",
      "* Epoch: 96, loss: 87.968681 \n",
      "* Epoch: 97, loss: 87.957108 \n",
      "* Epoch: 98, loss: 87.945366 \n",
      "* Epoch: 99, loss: 87.933510 \n",
      "* Epoch: 100, loss: 87.921997 \n",
      "* Epoch: 101, loss: 87.910530 \n",
      "* Epoch: 102, loss: 87.899345 \n",
      "* Epoch: 103, loss: 87.888138 \n",
      "* Epoch: 104, loss: 87.876961 \n",
      "* Epoch: 105, loss: 87.865776 \n",
      "* Epoch: 106, loss: 87.854675 \n",
      "* Epoch: 107, loss: 87.843719 \n",
      "* Epoch: 108, loss: 87.832901 \n",
      "* Epoch: 109, loss: 87.822151 \n",
      "* Epoch: 110, loss: 87.811317 \n",
      "* Epoch: 111, loss: 87.800499 \n",
      "* Epoch: 112, loss: 87.789948 \n",
      "* Epoch: 113, loss: 87.779282 \n",
      "* Epoch: 114, loss: 87.768959 \n",
      "* Epoch: 115, loss: 87.758690 \n",
      "* Epoch: 116, loss: 87.748451 \n",
      "* Epoch: 117, loss: 87.738319 \n",
      "* Epoch: 118, loss: 87.728340 \n",
      "* Epoch: 119, loss: 87.718414 \n",
      "* Epoch: 120, loss: 87.708267 \n",
      "* Epoch: 121, loss: 87.698608 \n",
      "* Epoch: 122, loss: 87.688873 \n",
      "* Epoch: 123, loss: 87.679024 \n",
      "* Epoch: 124, loss: 87.669418 \n",
      "* Epoch: 125, loss: 87.659889 \n",
      "* Epoch: 126, loss: 87.650528 \n",
      "* Epoch: 127, loss: 87.641060 \n",
      "* Epoch: 128, loss: 87.631493 \n",
      "* Epoch: 129, loss: 87.621948 \n",
      "* Epoch: 130, loss: 87.612564 \n",
      "* Epoch: 131, loss: 87.603249 \n",
      "* Epoch: 132, loss: 87.593895 \n",
      "* Epoch: 133, loss: 87.584625 \n",
      "* Epoch: 134, loss: 87.575584 \n",
      "* Epoch: 135, loss: 87.566353 \n",
      "* Epoch: 136, loss: 87.557434 \n",
      "* Epoch: 137, loss: 87.548546 \n",
      "* Epoch: 138, loss: 87.539398 \n",
      "* Epoch: 139, loss: 87.530464 \n",
      "* Epoch: 140, loss: 87.521484 \n",
      "* Epoch: 141, loss: 87.512802 \n",
      "* Epoch: 142, loss: 87.504181 \n",
      "* Epoch: 143, loss: 87.495506 \n",
      "* Epoch: 144, loss: 87.486870 \n",
      "* Epoch: 145, loss: 87.478310 \n",
      "* Epoch: 146, loss: 87.469872 \n",
      "* Epoch: 147, loss: 87.461334 \n",
      "* Epoch: 148, loss: 87.452744 \n",
      "* Epoch: 149, loss: 87.444214 \n",
      "* Epoch: 150, loss: 87.435616 \n",
      "* Epoch: 151, loss: 87.427528 \n",
      "* Epoch: 152, loss: 87.419075 \n",
      "* Epoch: 153, loss: 87.410843 \n",
      "* Epoch: 154, loss: 87.402855 \n",
      "* Epoch: 155, loss: 87.394714 \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for inputs, outputs in train_ds:\n",
    "        train_step(inputs)\n",
    "        \n",
    "    print(\"* Epoch: %i, loss: %f \"%(epoch, train_loss.result()))\n",
    "    manager.save()\n",
    "    plot_recimg(save_dir, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
